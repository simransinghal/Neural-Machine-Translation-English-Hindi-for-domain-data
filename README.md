# Intuit Codewar

Build an NMT (Neural MT) system when training data (parallel sentences in the concerned source and target language) is available in a domain. However, such domain data is of small size. Machine learning is to be used in such a way that the small sized domain data can be combined with the large amount of general data.

Contributor:
1) Arushi Singhal
2) Simran Singhal
3) Pujitha E
4) Srishti

# References
1) https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html (main)
2) https://arxiv.org/abs/1409.3215 (Research Paper)
3) http://www.manythings.org/anki/
4) https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/
5) https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/
6) https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/
7) http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
8) https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72
9) https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1
10) https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf
11) https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/
12) https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
13) https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
14) https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/ (important)
15) https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
16) https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb
17) https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7
18) https://discuss.pytorch.org/t/are-the-outputs-of-bidirectional-gru-concatenated/15103
19) https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53
20) https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb
21) https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66
22) https://discuss.pytorch.org/t/cuda-changes-expected-lstm-hidden-dimensions/10765/6
23) https://discuss.pytorch.org/t/cuda-changes-expected-lstm-hidden-dimensions/10765/6
24) https://github.com/A-Jacobson/minimal-nmt/blob/master/nmt_tutorial.ipynb (Important)
25) https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76 (GloVe in pytorch)

# Dataset Link
http://10.3.1.91/~datashare/wat/
